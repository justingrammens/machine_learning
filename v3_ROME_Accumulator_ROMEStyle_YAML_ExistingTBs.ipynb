{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/justingrammens/machine_learning/blob/master/v3_ROME_Accumulator_ROMEStyle_YAML_ExistingTBs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45255391",
      "metadata": {
        "id": "45255391"
      },
      "source": [
        "# ROME-style RTL Feedback Loop — Accumulator (SRA)\n",
        "\n",
        "This notebook follows the structure/pattern of `ROME_Demo_HDHP_11_6_2025.ipynb`, adapted for the Accumulator project:\n",
        "\n",
        "- **Specs**: `/content/accum_module_specs.yaml`\n",
        "- **Existing testbenches**: `/content/<module>tb.v` (no TB generation)\n",
        "- Runs **each sub-module**, then builds/verifies **top-level** `accum_core` last.\n",
        "- Leaf modules compile **only their own RTL**; top-level compiles **all generated RTL**.\n",
        "\n",
        "**Failure policy:**\n",
        "- If a **leaf module fails**, the notebook **continues** to the next leaf module.\n",
        "- If **any** leaf modules fail, the notebook **does not attempt** to compile/run the top-level module.\n",
        "\n",
        "Expected files in Colab `/content`:\n",
        "- `accum_module_specs.yaml`\n",
        "- `accum_enable_edgetb.v`, `accum_signal_bus_iftb.v`, …, `accum_coretb.v`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd62cc81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd62cc81",
        "outputId": "212bcef7-023c-47a8-922c-a3ec489c89e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/388.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Selecting previously unselected package iverilog.\n",
            "(Reading database ... 117528 files and directories currently installed.)\n",
            "Preparing to unpack .../iverilog_11.0-1.1_amd64.deb ...\n",
            "Unpacking iverilog (11.0-1.1) ...\n",
            "Setting up iverilog (11.0-1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "#@title Setting up the notebook\n",
        "\n",
        "!pip -q install openai anthropic pyyaml\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq iverilog\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf4fe211",
      "metadata": {
        "id": "bf4fe211"
      },
      "outputs": [],
      "source": [
        "#@title Select Model\n",
        "\n",
        "# OpenAI example: GPT-5.2 (replace with the model id available to your account if needed)\n",
        "model_choice = \"gpt-5.2\"\n",
        "\n",
        "# model_choice = \"claude-3-7-sonnet-20250219\"\n",
        "\n",
        "# Provider wrapper used by generate_verilog()\n",
        "# \"ChatGPT\" -> OpenAI (Colab secret 'ROME-Colab' or env OPENAI_API_KEY)\n",
        "# \"Claude\"  -> Anthropic (env CLAUDE_API_KEY)\n",
        "import os\n",
        "os.environ[\"MODEL\"] = \"ChatGPT\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccb080ae",
      "metadata": {
        "id": "ccb080ae"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions (ROME-style)\n",
        "\n",
        "import os, re, time, subprocess\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "import openai\n",
        "import anthropic\n",
        "\n",
        "try:\n",
        "    from google.colab import userdata\n",
        "except Exception:\n",
        "    userdata = None\n",
        "\n",
        "class Conversation:\n",
        "    def __init__(self, log_file=None):\n",
        "        self.messages = []\n",
        "        self.log_file = log_file\n",
        "        if self.log_file:\n",
        "            Path(self.log_file).write_text(\"\")\n",
        "\n",
        "    def add_message(self, role, content):\n",
        "        self.messages.append({\"role\": role, \"content\": content})\n",
        "        if self.log_file:\n",
        "            with open(self.log_file, \"a\") as f:\n",
        "                f.write(f\"\\n[{role.upper()}]\\n{content}\\n\")\n",
        "\n",
        "class ChatGPT:\n",
        "    def __init__(self):\n",
        "        api_key = None\n",
        "        if userdata is not None:\n",
        "            try:\n",
        "                api_key = userdata.get(\"ROME-Colab\")\n",
        "            except Exception:\n",
        "                api_key = None\n",
        "        api_key = api_key or os.environ.get(\"OPENAI_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Missing OpenAI API key. Set Colab secret 'ROME-Colab' or env OPENAI_API_KEY.\")\n",
        "        self.client = openai.OpenAI(api_key=api_key)\n",
        "\n",
        "    def generate(self, conv, model_id=\"\"):\n",
        "        resp = self.client.chat.completions.create(\n",
        "            model=model_id,\n",
        "            messages=conv.messages\n",
        "        )\n",
        "        return resp.choices[0].message.content\n",
        "\n",
        "class Claude:\n",
        "    def __init__(self):\n",
        "        api_key = os.environ.get(\"CLAUDE_API_KEY\")\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Missing CLAUDE_API_KEY.\")\n",
        "        self.client = anthropic.Anthropic(api_key=api_key)\n",
        "\n",
        "    def generate(self, conv, model_id=\"\"):\n",
        "        system_msgs = [m[\"content\"] for m in conv.messages if m[\"role\"] == \"system\"]\n",
        "        system = system_msgs[-1] if system_msgs else \"\"\n",
        "        user_msgs = [{\"role\": \"user\", \"content\": m[\"content\"]} for m in conv.messages if m[\"role\"] != \"system\"]\n",
        "        msg = self.client.messages.create(\n",
        "            model=model_id,\n",
        "            system=system,\n",
        "            messages=user_msgs,\n",
        "            max_tokens=4096\n",
        "        )\n",
        "        out = []\n",
        "        for b in msg.content:\n",
        "            if getattr(b, \"type\", None) == \"text\":\n",
        "                out.append(b.text)\n",
        "        return \"\\n\".join(out).strip()\n",
        "\n",
        "def generate_verilog(conv, model_type, model_id=\"\"):\n",
        "    if model_type == \"ChatGPT\":\n",
        "        model = ChatGPT()\n",
        "    elif model_type == \"Claude\":\n",
        "        model = Claude()\n",
        "    else:\n",
        "        raise ValueError(\"Invalid model type\")\n",
        "    return model.generate(conv, model_id=model_id)\n",
        "\n",
        "def extract_named_module(text, module_name):\n",
        "    text = re.sub(r\"```.*?\\n\", \"\", text, flags=re.DOTALL).replace(\"```\", \"\")\n",
        "    pat = rf\"\\bmodule\\s+{re.escape(module_name)}\\b.*?\\bendmodule\\b\"\n",
        "    m = re.search(pat, text, flags=re.DOTALL)\n",
        "    if m:\n",
        "        return m.group(0).strip() + \"\\n\"\n",
        "    any_mod = re.search(r\"\\bmodule\\b.*?\\bendmodule\\b\", text, flags=re.DOTALL)\n",
        "    return (any_mod.group(0).strip() + \"\\n\") if any_mod else (text.strip() + \"\\n\")\n",
        "\n",
        "def write_code_to_file(code, out_path: Path):\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_path.write_text(code)\n",
        "\n",
        "def load_specs(spec_path: Path):\n",
        "    spec = yaml.safe_load(spec_path.read_text())\n",
        "    mods_by_name = {m[\"name\"]: m for m in spec[\"modules\"]}\n",
        "    order = list(spec.get(\"module_generation_order\", list(mods_by_name.keys())))\n",
        "    return spec, mods_by_name, order\n",
        "\n",
        "def ports_to_ansi_list(ports_dict):\n",
        "    items = []\n",
        "    for pname, pdir in ports_dict.items():\n",
        "        pdir = str(pdir).split(\"#\")[0].strip()\n",
        "        items.append(f\"{pdir} {pname}\")\n",
        "    return \", \".join(items)\n",
        "\n",
        "def spec_prompt_for_module(mod_spec):\n",
        "    name = mod_spec[\"name\"]\n",
        "    intent = mod_spec.get(\"intent\", \"\")\n",
        "    ports = \"\\n\".join([f\"- {str(v).split('#')[0].strip()} {k}\" for k,v in mod_spec[\"ports\"].items()])\n",
        "    behavior = \"\\n\".join([f\"- {b}\" for b in mod_spec.get(\"behavior\", [])])\n",
        "    notes = mod_spec.get(\"notes\", [])\n",
        "    notes_txt = \"\\n\".join([f\"- {n}\" for n in notes]) if notes else \"(none)\"\n",
        "    return f\"\"\"// MODULE SPECIFICATION: {name}\n",
        "// INTENT: {intent}\n",
        "//\n",
        "// PORTS:\n",
        "// {ports}\n",
        "//\n",
        "// BEHAVIOR:\n",
        "// {behavior}\n",
        "//\n",
        "// NOTES:\n",
        "// {notes_txt}\n",
        "\"\"\"\n",
        "\n",
        "def iverilog_build(out_exe: Path, rtl_files, tb_file: Path, inc_dirs=None):\n",
        "    inc_dirs = inc_dirs or []\n",
        "    cmd = [\"iverilog\", \"-g2012\", \"-o\", str(out_exe)]\n",
        "    for d in inc_dirs:\n",
        "        cmd += [\"-I\", str(d)]\n",
        "    cmd += [str(p) for p in rtl_files] + [str(tb_file)]\n",
        "    return subprocess.run(cmd, capture_output=True, text=True)\n",
        "\n",
        "def vvp_run(exe: Path):\n",
        "    return subprocess.run([\"vvp\", str(exe)], capture_output=True, text=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3495f76f",
      "metadata": {
        "id": "3495f76f"
      },
      "outputs": [],
      "source": [
        "#@title Feedback Loop (uses existing TBs)\n",
        "\n",
        "def verilog_loop(design_prompt,\n",
        "                 module_name,\n",
        "                 rtl_out_path: Path,\n",
        "                 tb_path: Path,\n",
        "                 rtl_files_to_compile,\n",
        "                 max_iterations,\n",
        "                 model_type,\n",
        "                 model_id,\n",
        "                 log_path: Path,\n",
        "                 build_dir: Path):\n",
        "\n",
        "    conv = Conversation(log_file=str(log_path))\n",
        "    conv.add_message(\"system\",\n",
        "        \"You are an autocomplete engine for Verilog code. \"\n",
        "        \"Given a Verilog module specification, provide a completed synthesizable Verilog/SystemVerilog module. \"\n",
        "        \"Do not generate explanations. Do not generate test benches. \"\n",
        "        \"Return ONLY code for the requested module. \"\n",
        "        \"No delays, no force/release. Use nonblocking assignments for sequential logic.\"\n",
        "    )\n",
        "    conv.add_message(\"user\", design_prompt)\n",
        "\n",
        "    last_rtl = \"\"\n",
        "    total_start = time.time()\n",
        "    gen_time = 0.0\n",
        "    err_time = 0.0\n",
        "\n",
        "    for it in range(1, max_iterations + 1):\n",
        "        print(f\"\\n[{module_name}] iteration {it}/{max_iterations}\")\n",
        "\n",
        "        t0 = time.time()\n",
        "        response = generate_verilog(conv, model_type=model_type, model_id=model_id)\n",
        "        gen_time += (time.time() - t0)\n",
        "\n",
        "        rtl = extract_named_module(response, module_name)\n",
        "        last_rtl = rtl\n",
        "        write_code_to_file(rtl, rtl_out_path)\n",
        "\n",
        "        exe = build_dir / f\"sim_{module_name}.out\"\n",
        "        proc = iverilog_build(exe, rtl_files_to_compile, tb_path, inc_dirs=[tb_path.parent, rtl_out_path.parent])\n",
        "\n",
        "        if proc.returncode != 0:\n",
        "            err_time_start = time.time()\n",
        "            msg = (\n",
        "                \"The RTL or testbench failed to compile.\\n\"\n",
        "                \"iverilog stderr:\\n\" + proc.stderr + \"\\n\"\n",
        "                \"iverilog stdout:\\n\" + proc.stdout + \"\\n\"\n",
        "                \"Please correct ONLY the RTL for module \" + module_name + \" and output the full corrected module.\"\n",
        "            )\n",
        "            print(\"Compile failed.\")\n",
        "            conv.add_message(\"assistant\", response)\n",
        "            conv.add_message(\"user\", msg + \"\\n\\nCurrent RTL:\\n\" + last_rtl)\n",
        "            err_time += (time.time() - err_time_start)\n",
        "            continue\n",
        "\n",
        "        run = vvp_run(exe)\n",
        "        sim_out = (run.stdout or \"\") + (run.stderr or \"\")\n",
        "\n",
        "        if run.returncode != 0 or (\"passed!\" not in sim_out):\n",
        "            err_time_start = time.time()\n",
        "            msg = (\n",
        "                \"The testbench did not pass.\\n\"\n",
        "                \"Simulation output:\\n\" + sim_out + \"\\n\"\n",
        "                \"Please correct ONLY the RTL for module \" + module_name + \" and output the full corrected module.\"\n",
        "            )\n",
        "            print(\"Simulation failed or did not pass.\")\n",
        "            conv.add_message(\"assistant\", response)\n",
        "            conv.add_message(\"user\", msg + \"\\n\\nCurrent RTL:\\n\" + last_rtl)\n",
        "            err_time += (time.time() - err_time_start)\n",
        "            continue\n",
        "\n",
        "        print(f\"{module_name}: passed!\")\n",
        "        total_time = time.time() - total_start\n",
        "        return total_time, gen_time, err_time, True\n",
        "\n",
        "    total_time = time.time() - total_start\n",
        "    return total_time, gen_time, err_time, False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "317a2bbf",
      "metadata": {
        "id": "317a2bbf"
      },
      "outputs": [],
      "source": [
        "#@title Hierarchical Loop (YAML → leaf modules → top-level) with non-stop leaf failures\n",
        "\n",
        "def hier_gen_from_yaml(spec_path: Path,\n",
        "                       tb_dir: Path,\n",
        "                       out_root: Path,\n",
        "                       top_module: str = \"accum_core\",\n",
        "                       max_iterations: int = 10):\n",
        "\n",
        "    spec, mods_by_name, yaml_order = load_specs(spec_path)\n",
        "\n",
        "    def tb_for(m): return tb_dir / f\"{m}tb.v\"\n",
        "    modules_with_tb = [m for m in yaml_order if tb_for(m).exists()]\n",
        "\n",
        "    leaf = [m for m in modules_with_tb if m != top_module]\n",
        "    have_top_tb = top_module in modules_with_tb\n",
        "\n",
        "    print(\"Leaf modules:\")\n",
        "    for m in leaf:\n",
        "        print(\" -\", m, \"->\", tb_for(m))\n",
        "    print(\"\\nTop module:\", top_module, \"->\", tb_for(top_module) if have_top_tb else \"(no tb found)\")\n",
        "\n",
        "    model_type = os.environ.get(\"MODEL\", \"ChatGPT\")\n",
        "\n",
        "    # Track results without stopping on leaf failures\n",
        "    results = {}\n",
        "    times = {}\n",
        "\n",
        "    generated_paths = {}  # only successful modules will be included here\n",
        "\n",
        "    # ----------\n",
        "    # 1) Run ALL leaves\n",
        "    # ----------\n",
        "    for m in leaf:\n",
        "        mod_spec = mods_by_name[m]\n",
        "        tb_path = tb_for(m)\n",
        "\n",
        "        mod_dir = out_root / m\n",
        "        mod_dir.mkdir(parents=True, exist_ok=True)\n",
        "        rtl_out = mod_dir / f\"{m}.v\"\n",
        "        log_path = mod_dir / \"log.txt\"\n",
        "        build_dir = mod_dir / \"build\"\n",
        "        build_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        prompt = spec_prompt_for_module(mod_spec)\n",
        "        ansi_ports = ports_to_ansi_list(mod_spec[\"ports\"])\n",
        "        prompt += f\"\\n// Please implement:\\nmodule {m}({ansi_ports});\\n// Insert code here\\nendmodule\\n\"\n",
        "\n",
        "        # Leaf compiles ONLY its own RTL + TB\n",
        "        rtl_files = [rtl_out]\n",
        "\n",
        "        total, gen, err, ok = verilog_loop(\n",
        "            design_prompt=prompt,\n",
        "            module_name=m,\n",
        "            rtl_out_path=rtl_out,\n",
        "            tb_path=tb_path,\n",
        "            rtl_files_to_compile=rtl_files,\n",
        "            max_iterations=max_iterations,\n",
        "            model_type=model_type,\n",
        "            model_id=model_choice,\n",
        "            log_path=log_path,\n",
        "            build_dir=build_dir\n",
        "        )\n",
        "\n",
        "        results[m] = ok\n",
        "        times[m] = {\"total\": total, \"gen\": gen, \"err\": err}\n",
        "\n",
        "        if ok:\n",
        "            generated_paths[m] = rtl_out\n",
        "\n",
        "    failed_leaf = [m for m in leaf if not results.get(m, False)]\n",
        "\n",
        "    print(\"\\nLeaf results:\")\n",
        "    for m in leaf:\n",
        "        print(f\" - {m}: {'PASS' if results[m] else 'FAIL'}\")\n",
        "\n",
        "    # ----------\n",
        "    # 2) Top-level policy: only attempt top if ALL leaves passed and top TB exists\n",
        "    # ----------\n",
        "    if failed_leaf:\n",
        "        print(\"\\nOne or more leaf modules FAILED:\")\n",
        "        for m in failed_leaf:\n",
        "            print(\" -\", m)\n",
        "        print(\"\\nPer policy: skipping top-level build/verification.\")\n",
        "        return {\"generated\": generated_paths, \"results\": results, \"times\": times, \"skipped_top\": True}\n",
        "\n",
        "    if not have_top_tb:\n",
        "        print(\"\\nTop-level TB not found; nothing to run for top-level.\")\n",
        "        return {\"generated\": generated_paths, \"results\": results, \"times\": times, \"skipped_top\": True}\n",
        "\n",
        "    # ----------\n",
        "    # 3) Run top-level\n",
        "    # ----------\n",
        "    m = top_module\n",
        "    mod_spec = mods_by_name[m]\n",
        "    tb_path = tb_for(m)\n",
        "\n",
        "    mod_dir = out_root / m\n",
        "    mod_dir.mkdir(parents=True, exist_ok=True)\n",
        "    rtl_out = mod_dir / f\"{m}.v\"\n",
        "    log_path = mod_dir / \"log.txt\"\n",
        "    build_dir = mod_dir / \"build\"\n",
        "    build_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    prompt = spec_prompt_for_module(mod_spec)\n",
        "    prompt += \"\\n// Previously generated leaf submodules (for reference):\\n\"\n",
        "    for sm, p in generated_paths.items():\n",
        "        prompt += f\"\\n// ---- {sm} ----\\n{p.read_text()}\\n\"\n",
        "\n",
        "    ansi_ports = ports_to_ansi_list(mod_spec[\"ports\"])\n",
        "    prompt += f\"\\n// Please implement:\\nmodule {m}({ansi_ports});\\n// Insert code here\\nendmodule\\n\"\n",
        "\n",
        "    # Top compiles all PASS leaf RTL + top RTL\n",
        "    rtl_files = [generated_paths[x] for x in leaf] + [rtl_out]\n",
        "\n",
        "    total, gen, err, ok = verilog_loop(\n",
        "        design_prompt=prompt,\n",
        "        module_name=m,\n",
        "        rtl_out_path=rtl_out,\n",
        "        tb_path=tb_path,\n",
        "        rtl_files_to_compile=rtl_files,\n",
        "        max_iterations=max_iterations,\n",
        "        model_type=model_type,\n",
        "        model_id=model_choice,\n",
        "        log_path=log_path,\n",
        "        build_dir=build_dir\n",
        "    )\n",
        "\n",
        "    results[m] = ok\n",
        "    times[m] = {\"total\": total, \"gen\": gen, \"err\": err}\n",
        "\n",
        "    if ok:\n",
        "        generated_paths[m] = rtl_out\n",
        "\n",
        "    print(\"\\nTop-level result: \", \"PASS\" if ok else \"FAIL\")\n",
        "    return {\"generated\": generated_paths, \"results\": results, \"times\": times, \"skipped_top\": False}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f6ccd3",
      "metadata": {
        "id": "75f6ccd3"
      },
      "source": [
        "# Setting the API Key\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e742a8cc",
      "metadata": {
        "id": "e742a8cc"
      },
      "outputs": [],
      "source": [
        "#@title API Key setup\n",
        "\n",
        "# Option A (recommended): Colab secret\n",
        "#   - Add a secret named: ROME-Colab\n",
        "#   - Put your OpenAI API key in it\n",
        "#\n",
        "# Option B: environment variable (uncomment)\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "\n",
        "import os\n",
        "# Fetch API key ONCE and store in environment variable\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"ROME-Colab\")\n",
        "os.environ[\"MODEL\"] = \"ChatGPT\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc8dad20",
      "metadata": {
        "id": "dc8dad20"
      },
      "source": [
        "# Verify the YAML + testbenches are present\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50fd0e2a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50fd0e2a",
        "outputId": "e4b6254a-2446-4df2-9c95-af1caf004b73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found TBs:\n",
            " - accum_coretb.v\n",
            " - accum_irq_gentb.v\n",
            " - accum_len_countertb.v\n",
            " - accum_rectifiertb.v\n",
            " - accum_signal_bus_iftb.v\n",
            " - accum_sra_postproctb.v\n",
            " - accum_sum_datapathtb.v\n",
            " - accum_warmup_ctrltb.v\n",
            "\n",
            "Modules in YAML order (tb present?):\n",
            " - accum_enable_edge NO\n",
            " - accum_signal_bus_if YES\n",
            " - accum_rectifier YES\n",
            " - accum_warmup_ctrl YES\n",
            " - accum_len_counter YES\n",
            " - accum_sum_datapath YES\n",
            " - accum_sra_postproc YES\n",
            " - accum_irq_gen YES\n",
            " - accum_core YES\n"
          ]
        }
      ],
      "source": [
        "#@title Verify files in /content\n",
        "\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "\n",
        "SPEC_PATH = Path(\"/content/accum_module_specs.yaml\")\n",
        "TB_DIR    = Path(\"/content\")\n",
        "OUT_ROOT  = Path(\"/content/generated_accum\")\n",
        "\n",
        "assert SPEC_PATH.exists(), f\"Missing YAML: {SPEC_PATH}\"\n",
        "\n",
        "tbs = sorted(TB_DIR.glob(\"*tb.v\"))\n",
        "print(\"Found TBs:\")\n",
        "for p in tbs:\n",
        "    print(\" -\", p.name)\n",
        "\n",
        "spec = yaml.safe_load(SPEC_PATH.read_text())\n",
        "order = spec.get(\"module_generation_order\", [m[\"name\"] for m in spec[\"modules\"]])\n",
        "\n",
        "print(\"\\nModules in YAML order (tb present?):\")\n",
        "for m in order:\n",
        "    print(\" -\", m, \"YES\" if (TB_DIR / f\"{m}tb.v\").exists() else \"NO\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a2883a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82a2883a",
        "outputId": "d01cff10-61a4-4418-ae81-0b2e5c082ccb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leaf modules:\n",
            " - accum_signal_bus_if -> /content/accum_signal_bus_iftb.v\n",
            " - accum_rectifier -> /content/accum_rectifiertb.v\n",
            " - accum_warmup_ctrl -> /content/accum_warmup_ctrltb.v\n",
            " - accum_len_counter -> /content/accum_len_countertb.v\n",
            " - accum_sum_datapath -> /content/accum_sum_datapathtb.v\n",
            " - accum_sra_postproc -> /content/accum_sra_postproctb.v\n",
            " - accum_irq_gen -> /content/accum_irq_gentb.v\n",
            "\n",
            "Top module: accum_core -> /content/accum_coretb.v\n",
            "\n",
            "[accum_signal_bus_if] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_signal_bus_if] iteration 2/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 3/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 4/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 5/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 6/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 7/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 8/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 9/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_signal_bus_if] iteration 10/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_rectifier] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_rectifier] iteration 2/10\n",
            "accum_rectifier: passed!\n",
            "\n",
            "[accum_warmup_ctrl] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_warmup_ctrl] iteration 2/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_warmup_ctrl] iteration 3/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_warmup_ctrl] iteration 4/10\n",
            "accum_warmup_ctrl: passed!\n",
            "\n",
            "[accum_len_counter] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_len_counter] iteration 2/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_len_counter] iteration 3/10\n",
            "accum_len_counter: passed!\n",
            "\n",
            "[accum_sum_datapath] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_sum_datapath] iteration 2/10\n",
            "accum_sum_datapath: passed!\n",
            "\n",
            "[accum_sra_postproc] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_sra_postproc] iteration 2/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_sra_postproc] iteration 3/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 4/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 5/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 6/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 7/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 8/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 9/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_sra_postproc] iteration 10/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_irq_gen] iteration 1/10\n",
            "Compile failed.\n",
            "\n",
            "[accum_irq_gen] iteration 2/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_irq_gen] iteration 3/10\n",
            "Simulation failed or did not pass.\n",
            "\n",
            "[accum_irq_gen] iteration 4/10\n",
            "accum_irq_gen: passed!\n",
            "\n",
            "Leaf results:\n",
            " - accum_signal_bus_if: FAIL\n",
            " - accum_rectifier: PASS\n",
            " - accum_warmup_ctrl: PASS\n",
            " - accum_len_counter: PASS\n",
            " - accum_sum_datapath: PASS\n",
            " - accum_sra_postproc: FAIL\n",
            " - accum_irq_gen: PASS\n",
            "\n",
            "One or more leaf modules FAILED:\n",
            " - accum_signal_bus_if\n",
            " - accum_sra_postproc\n",
            "\n",
            "Per policy: skipping top-level build/verification.\n",
            "\n",
            "Run summary:\n",
            "Skipped top: True\n",
            "Results:\n",
            " - accum_signal_bus_if: FAIL\n",
            " - accum_rectifier: PASS\n",
            " - accum_warmup_ctrl: PASS\n",
            " - accum_len_counter: PASS\n",
            " - accum_sum_datapath: PASS\n",
            " - accum_sra_postproc: FAIL\n",
            " - accum_irq_gen: PASS\n",
            "\n",
            "Generated RTL files:\n",
            " - accum_rectifier -> /content/generated_accum/accum_rectifier/accum_rectifier.v\n",
            " - accum_warmup_ctrl -> /content/generated_accum/accum_warmup_ctrl/accum_warmup_ctrl.v\n",
            " - accum_len_counter -> /content/generated_accum/accum_len_counter/accum_len_counter.v\n",
            " - accum_sum_datapath -> /content/generated_accum/accum_sum_datapath/accum_sum_datapath.v\n",
            " - accum_irq_gen -> /content/generated_accum/accum_irq_gen/accum_irq_gen.v\n"
          ]
        }
      ],
      "source": [
        "#@title Run the hierarchical generation loop\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"MODEL\"] = \"ChatGPT\"  # or \"Claude\"\n",
        "\n",
        "run = hier_gen_from_yaml(\n",
        "    spec_path=Path(\"/content/accum_module_specs.yaml\"),\n",
        "    tb_dir=Path(\"/content\"),\n",
        "    out_root=Path(\"/content/generated_accum\"),\n",
        "    top_module=\"accum_core\",\n",
        "    max_iterations=10\n",
        ")\n",
        "\n",
        "print(\"\\nRun summary:\")\n",
        "print(\"Skipped top:\", run.get(\"skipped_top\"))\n",
        "print(\"Results:\")\n",
        "for k,v in run[\"results\"].items():\n",
        "    print(f\" - {k}: {'PASS' if v else 'FAIL'}\")\n",
        "\n",
        "print(\"\\nGenerated RTL files:\")\n",
        "for k,p in run[\"generated\"].items():\n",
        "    print(\" -\", k, \"->\", p)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "id": "sXzqqwWUu8ep",
        "outputId": "d63fb15e-02d3-4da6-977b-74038d453432"
      },
      "id": "sXzqqwWUu8ep",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}